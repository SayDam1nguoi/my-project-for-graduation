// Interview Analysis System - Full Features
// JavaScript for all tabs

const API_URL = 'http://localhost:8000';

// ===== TAB SWITCHING =====
function switchTab(tabName) {
    // Hide all tabs
    document.querySelectorAll('.tab-content').forEach(tab => {
        tab.classList.remove('active');
    });

    // Remove active from all buttons
    document.querySelectorAll('.tab-button').forEach(btn => {
        btn.classList.remove('active');
    });

    // Show selected tab
    document.getElementById(`tab-${tabName}`).classList.add('active');

    // Activate button
    event.target.classList.add('active');
}

// ===== UTILITY FUNCTIONS =====
function showStatus(elementId, message, type = 'info') {
    const statusEl = document.getElementById(elementId);
    statusEl.textContent = message;
    statusEl.className = `status-message show ${type}`;
}

function hideStatus(elementId) {
    const statusEl = document.getElementById(elementId);
    statusEl.classList.remove('show');
}

function setupFileInput(inputId, fileNameId, buttonId, uploadAreaId) {
    const input = document.getElementById(inputId);
    const fileName = document.getElementById(fileNameId);
    const button = document.getElementById(buttonId);
    const uploadArea = document.getElementById(uploadAreaId);

    // File input change
    input.addEventListener('change', (e) => {
        const file = e.target.files[0];
        if (file) {
            fileName.textContent = `üìπ ${file.name}`;
            button.disabled = false;
        }
    });

    // Drag and drop
    uploadArea.addEventListener('dragover', (e) => {
        e.preventDefault();
        uploadArea.classList.add('dragover');
    });

    uploadArea.addEventListener('dragleave', () => {
        uploadArea.classList.remove('dragover');
    });

    uploadArea.addEventListener('drop', (e) => {
        e.preventDefault();
        uploadArea.classList.remove('dragover');

        const files = e.dataTransfer.files;
        if (files.length > 0) {
            input.files = files;
            fileName.textContent = `üìπ ${files[0].name}`;
            button.disabled = false;
        }
    });
}

// ===== TAB 1: EMOTION RECOGNITION =====
setupFileInput('emotionVideoInput', 'emotionFileName', 'emotionAnalyzeBtn', 'emotionUploadArea');

document.getElementById('emotionAnalyzeBtn').addEventListener('click', async () => {
    const input = document.getElementById('emotionVideoInput');
    const file = input.files[0];
    if (!file) return;

    showStatus('emotionStatus', 'üîÑ ƒêang ph√¢n t√≠ch c·∫£m x√∫c... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)', 'info');
    document.getElementById('emotionResults').style.display = 'none';

    try {
        const formData = new FormData();
        formData.append('file', file);

        const response = await fetch(`${API_URL}/api/analyze-sync`, {
            method: 'POST',
            body: formData
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}`);
        }

        const result = await response.json();

        // Display emotion results
        displayEmotionResults(result);

        // Also display focus details if available
        if (result.details && result.details.focus) {
            displayFocusDetails(result.details.focus);
        }

        showStatus('emotionStatus', '‚úÖ Ph√¢n t√≠ch ho√†n t·∫•t!', 'success');

    } catch (error) {
        console.error('Error:', error);
        showStatus('emotionStatus', `‚ùå L·ªói: ${error.message}`, 'error');
    }
});

function displayEmotionResults(result) {
    const resultsDiv = document.getElementById('emotionResults');
    const gridDiv = document.getElementById('emotionResultsGrid');

    gridDiv.innerHTML = `
        <div class="result-card">
            <h3>üòä ƒêi·ªÉm C·∫£m X√∫c</h3>
            <div class="score">${result.scores.emotion.toFixed(1)}</div>
        </div>
        <div class="result-card">
            <h3>üëÅÔ∏è ƒêi·ªÉm T·∫≠p Trung</h3>
            <div class="score">${result.scores.focus.toFixed(1)}</div>
        </div>
        <div class="result-card">
            <h3>üìä ƒêi·ªÉm T·ªïng</h3>
            <div class="score">${result.scores.total.toFixed(1)}</div>
        </div>
        <div class="result-card">
            <h3>‚≠ê ƒê√°nh Gi√°</h3>
            <div class="score" style="font-size: 1.5em;">${result.rating}</div>
        </div>
    `;

    resultsDiv.style.display = 'block';
}

function displayFocusDetails(focusDetails) {
    // Display focus details if available
    if (!focusDetails) return;

    const resultsDiv = document.getElementById('emotionResults');

    // Add focus details section
    const focusSection = document.createElement('div');
    focusSection.style.marginTop = '20px';
    focusSection.style.padding = '15px';
    focusSection.style.background = '#252525';
    focusSection.style.borderRadius = '10px';

    focusSection.innerHTML = `
        <h3 style="color: #667eea; margin-bottom: 15px;">üìä Chi Ti·∫øt T·∫≠p Trung</h3>
        <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px;">
            <div>
                <strong>‚è±Ô∏è Th·ªùi gian t·∫≠p trung:</strong><br>
                ${focusDetails.focused_time || 0}s (${focusDetails.focused_rate || 0}%)
            </div>
            <div>
                <strong>‚ö†Ô∏è Th·ªùi gian m·∫•t t·∫≠p trung:</strong><br>
                ${focusDetails.distracted_time || 0}s (${focusDetails.distracted_rate || 0}%)
            </div>
            <div>
                <strong>üî¢ S·ªë l·∫ßn m·∫•t t·∫≠p trung:</strong><br>
                ${focusDetails.distracted_count || 0} l·∫ßn
            </div>
            <div>
                <strong>üìà ƒêi·ªÉm trung b√¨nh:</strong><br>
                ${focusDetails.average_attention || 0}/10
            </div>
        </div>
    `;

    resultsDiv.appendChild(focusSection);
}

// ===== TAB 2: VIDEO TRANSCRIPTION =====
setupFileInput('videoInput', 'videoFileName', 'videoTranscribeBtn', 'videoUploadArea');

document.getElementById('videoTranscribeBtn').addEventListener('click', async () => {
    const input = document.getElementById('videoInput');
    const file = input.files[0];
    if (!file) return;

    showStatus('videoStatus', 'üîÑ ƒêang chuy·ªÉn ƒë·ªïi audio trong video sang text... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)', 'info');
    document.getElementById('videoTranscript').style.display = 'none';

    try {
        const formData = new FormData();
        formData.append('file', file);

        // Call transcription endpoint
        const response = await fetch(`${API_URL}/api/transcribe-video`, {
            method: 'POST',
            body: formData
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}`);
        }

        const result = await response.json();

        // Display transcript
        const transcriptText = result.transcript_with_timestamps || result.transcript || 'Kh√¥ng c√≥ transcript';
        const wordCount = result.word_count || 0;
        const duration = result.duration || 0;
        const langDisplay = result.language_display || result.language || 'unknown';
        const segments = result.segments || 0;

        document.getElementById('videoTranscriptText').innerHTML = `
            <div style="margin-bottom: 15px; padding: 10px; background: #252525; border-radius: 5px;">
                <strong>üìä Th√¥ng tin:</strong><br>
                S·ªë t·ª´: ${wordCount} | Th·ªùi l∆∞·ª£ng: ${duration.toFixed(1)}s | Ng√¥n ng·ªØ: ${langDisplay} | Segments: ${segments}
            </div>
            <div style="white-space: pre-wrap;">${transcriptText}</div>
        `;

        document.getElementById('videoTranscript').style.display = 'block';
        showStatus('videoStatus', '‚úÖ Chuy·ªÉn ƒë·ªïi ho√†n t·∫•t!', 'success');

    } catch (error) {
        console.error('Error:', error);
        showStatus('videoStatus', `‚ùå L·ªói: ${error.message}. Ki·ªÉm tra video c√≥ audio kh√¥ng?`, 'error');
    }
});

// ===== TAB 3: AUDIO TRANSCRIPTION =====
setupFileInput('audioInput', 'audioFileName', 'audioTranscribeBtn', 'audioUploadArea');

document.getElementById('audioTranscribeBtn').addEventListener('click', async () => {
    const input = document.getElementById('audioInput');
    const file = input.files[0];
    if (!file) return;

    showStatus('audioStatus', 'üîÑ ƒêang chuy·ªÉn ƒë·ªïi audio sang text...', 'info');
    document.getElementById('audioTranscript').style.display = 'none';

    try {
        const formData = new FormData();
        formData.append('file', file);

        // Call audio transcription endpoint
        const response = await fetch(`${API_URL}/api/transcribe-audio`, {
            method: 'POST',
            body: formData
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}`);
        }

        const result = await response.json();

        // Display transcript
        const transcriptText = result.transcript || 'Kh√¥ng c√≥ transcript';
        const wordCount = result.word_count || 0;

        document.getElementById('audioTranscriptText').innerHTML = `
            <div style="margin-bottom: 15px; padding: 10px; background: #252525; border-radius: 5px;">
                <strong>üìä Th√¥ng tin:</strong><br>
                S·ªë t·ª´: ${wordCount}
            </div>
            <div style="white-space: pre-wrap;">${transcriptText}</div>
        `;

        document.getElementById('audioTranscript').style.display = 'block';
        showStatus('audioStatus', '‚úÖ Chuy·ªÉn ƒë·ªïi ho√†n t·∫•t!', 'success');

    } catch (error) {
        console.error('Error:', error);
        showStatus('audioStatus', `‚ùå L·ªói: ${error.message}`, 'error');
    }
});

document.getElementById('audioTranscribeBtn').addEventListener('click', async () => {
    const input = document.getElementById('audioInput');
    const file = input.files[0];
    if (!file) return;

    showStatus('audioStatus', 'üîÑ ƒêang chuy·ªÉn ƒë·ªïi audio... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)', 'info');
    document.getElementById('audioTranscript').style.display = 'none';

    try {
        const formData = new FormData();
        formData.append('file', file);

        const response = await fetch(`${API_URL}/api/analyze-sync`, {
            method: 'POST',
            body: formData
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}`);
        }

        const result = await response.json();

        // Extract transcript
        const transcript = result.details?.clarity?.transcription_text ||
            result.details?.content?.transcript ||
            'Kh√¥ng c√≥ transcript';

        // Display transcript
        document.getElementById('audioTranscriptText').textContent = transcript;
        document.getElementById('audioTranscript').style.display = 'block';
        showStatus('audioStatus', '‚úÖ Chuy·ªÉn ƒë·ªïi ho√†n t·∫•t!', 'success');

    } catch (error) {
        console.error('Error:', error);
        showStatus('audioStatus', `‚ùå L·ªói: ${error.message}`, 'error');
    }
});

// Copy transcript function
function copyTranscript(type) {
    const textId = type === 'video' ? 'videoTranscriptText' : 'audioTranscriptText';
    const text = document.getElementById(textId).textContent;

    navigator.clipboard.writeText(text).then(() => {
        alert('‚úÖ ƒê√£ copy transcript!');
    }).catch(err => {
        console.error('Copy failed:', err);
        alert('‚ùå Kh√¥ng th·ªÉ copy');
    });
}

// ===== TAB 4: SCORE SUMMARY =====
setupFileInput('summaryVideoInput', 'summaryFileName', 'summaryAnalyzeBtn', 'summaryUploadArea');

// Weight controls
const weightInputs = ['weightEmotion', 'weightFocus', 'weightClarity', 'weightContent'];
weightInputs.forEach(id => {
    document.getElementById(id).addEventListener('input', updateTotalWeight);
});

function updateTotalWeight() {
    const total = weightInputs.reduce((sum, id) => {
        return sum + parseInt(document.getElementById(id).value || 0);
    }, 0);

    document.getElementById('totalWeight').textContent = total;

    // Highlight if not 100%
    const totalEl = document.getElementById('totalWeight');
    if (total !== 100) {
        totalEl.style.color = '#f5576c';
    } else {
        totalEl.style.color = '#43e97b';
    }
}

document.getElementById('summaryAnalyzeBtn').addEventListener('click', async () => {
    const input = document.getElementById('summaryVideoInput');
    const file = input.files[0];
    if (!file) return;

    // Check weights
    const total = weightInputs.reduce((sum, id) => {
        return sum + parseInt(document.getElementById(id).value || 0);
    }, 0);

    if (total !== 100) {
        showStatus('summaryStatus', '‚ö†Ô∏è T·ªïng tr·ªçng s·ªë ph·∫£i b·∫±ng 100%!', 'warning');
        return;
    }

    showStatus('summaryStatus', 'üîÑ ƒêang ph√¢n t√≠ch to√†n di·ªán... (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)', 'info');
    document.getElementById('summaryResults').style.display = 'none';

    try {
        const formData = new FormData();
        formData.append('file', file);

        const response = await fetch(`${API_URL}/api/analyze-sync`, {
            method: 'POST',
            body: formData
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}`);
        }

        const result = await response.json();

        // Get custom weights
        const weights = {
            emotion: parseInt(document.getElementById('weightEmotion').value) / 100,
            focus: parseInt(document.getElementById('weightFocus').value) / 100,
            clarity: parseInt(document.getElementById('weightClarity').value) / 100,
            content: parseInt(document.getElementById('weightContent').value) / 100
        };

        // Calculate custom total score
        const customTotal = (
            result.scores.emotion * weights.emotion +
            result.scores.focus * weights.focus +
            result.scores.clarity * weights.clarity +
            result.scores.content * weights.content
        );

        // Display results
        displaySummaryResults(result, customTotal);
        showStatus('summaryStatus', '‚úÖ Ph√¢n t√≠ch ho√†n t·∫•t!', 'success');

    } catch (error) {
        console.error('Error:', error);
        showStatus('summaryStatus', `‚ùå L·ªói: ${error.message}`, 'error');
    }
});

function displaySummaryResults(result, customTotal) {
    // Total score
    document.getElementById('summaryTotalScore').textContent = customTotal.toFixed(1);

    // Rating based on custom total
    let rating;
    if (customTotal >= 9.0) rating = 'XU·∫§T S·∫ÆC';
    else if (customTotal >= 8.0) rating = 'R·∫§T T·ªêT';
    else if (customTotal >= 7.0) rating = 'T·ªêT';
    else if (customTotal >= 6.0) rating = 'KH√Å';
    else if (customTotal >= 5.0) rating = 'TRUNG B√åNH';
    else rating = 'C·∫¶N C·∫¢I THI·ªÜN';

    document.getElementById('summaryRating').textContent = rating;

    // Individual scores
    document.getElementById('summaryEmotionScore').textContent = result.scores.emotion.toFixed(1);
    document.getElementById('summaryFocusScore').textContent = result.scores.focus.toFixed(1);
    document.getElementById('summaryClarityScore').textContent = result.scores.clarity.toFixed(1);
    document.getElementById('summaryContentScore').textContent = result.scores.content.toFixed(1);

    // Show results
    document.getElementById('summaryResults').style.display = 'block';
}

function resetSummary() {
    document.getElementById('summaryVideoInput').value = '';
    document.getElementById('summaryFileName').textContent = '';
    document.getElementById('summaryAnalyzeBtn').disabled = true;
    document.getElementById('summaryResults').style.display = 'none';
    hideStatus('summaryStatus');
}

// ===== TAB 0: REAL-TIME CAMERA =====
let cameraStream = null;
let cameraMediaRecorder = null;
let cameraRecordedChunks = [];
let cameraRecordingStartTime = null;
let cameraTimerInterval = null;
let faceApiModelsLoaded = false;
let detectionInterval = null;

// Focus tracking variables
let cameraFocusHistory = [];
let cameraTotalFocusedTime = 0;
let cameraTotalDistractedTime = 0;
let cameraDistractedEvents = 0;
let cameraCurrentlyDistracted = false;
let cameraStartTime = null;

// Load face-api.js models
async function loadFaceApiModels() {
    if (faceApiModelsLoaded) return true;

    try {
        showStatus('cameraStatusMsg', '‚è≥ ƒêang t·∫£i AI models... (ch·ªâ l·∫ßn ƒë·∫ßu, ~10-15 gi√¢y)', 'info');

        // Try multiple CDN sources
        const MODEL_URLS = [
            'https://cdn.jsdelivr.net/npm/@vladmandic/face-api@1.7.12/model',
            'https://justadudewhohacks.github.io/face-api.js/models'
        ];

        let loaded = false;
        for (const MODEL_URL of MODEL_URLS) {
            try {
                console.log(`Trying to load models from: ${MODEL_URL}`);

                await Promise.all([
                    faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
                    faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),  // C·∫¶N CHO HEAD POSE
                    faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)
                ]);

                loaded = true;
                console.log(`‚úÖ Face-api.js models loaded from: ${MODEL_URL}`);
                break;
            } catch (err) {
                console.warn(`Failed to load from ${MODEL_URL}:`, err);
                continue;
            }
        }

        if (!loaded) {
            throw new Error('Could not load models from any CDN');
        }

        faceApiModelsLoaded = true;
        return true;
    } catch (error) {
        console.error('‚ùå Failed to load models:', error);
        showStatus('cameraStatusMsg', '‚ùå Kh√¥ng th·ªÉ t·∫£i AI models. Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet.', 'error');
        return false;
    }
}

// Start Camera
document.getElementById('cameraStartBtn').addEventListener('click', async () => {
    try {
        // Load models first
        showStatus('cameraStatusMsg', '‚è≥ ƒêang kh·ªüi ƒë·ªông camera...', 'info');

        // Load models FIRST and WAIT
        const modelsLoaded = await loadFaceApiModels();
        if (!modelsLoaded) {
            throw new Error('Kh√¥ng th·ªÉ t·∫£i AI models. Vui l√≤ng th·ª≠ l·∫°i.');
        }

        // Request camera access
        cameraStream = await navigator.mediaDevices.getUserMedia({
            video: {
                width: { ideal: 1280 },
                height: { ideal: 720 },
                facingMode: 'user'
            },
            audio: true // Include audio for recording
        });

        const videoElement = document.getElementById('cameraVideo');
        videoElement.srcObject = cameraStream;

        // Wait for video to be ready
        await new Promise(resolve => {
            videoElement.onloadedmetadata = () => {
                resolve();
            };
        });

        // Update UI
        document.getElementById('cameraStartBtn').disabled = true;
        document.getElementById('cameraStopBtn').disabled = false;
        document.getElementById('cameraRecordBtn').disabled = false;
        document.getElementById('cameraStatus').textContent = 'ƒêang ho·∫°t ƒë·ªông';
        document.getElementById('cameraStatus').style.color = '#43e97b';

        showStatus('cameraStatusMsg', '‚úÖ Camera ƒë√£ b·∫≠t! ƒêang ph√°t hi·ªán c·∫£m x√∫c real-time...', 'success');

        // Start real-time face detection with LAUNCHER FORMULA
        // Wait a bit for video to stabilize
        setTimeout(() => {
            startRealTimeFaceDetection_Launcher();
        }, 500);

    } catch (error) {
        console.error('Camera error:', error);
        showStatus('cameraStatusMsg', '‚ùå Kh√¥ng th·ªÉ truy c·∫≠p camera. Vui l√≤ng cho ph√©p quy·ªÅn truy c·∫≠p.', 'error');
    }
});

// Stop Camera
document.getElementById('cameraStopBtn').addEventListener('click', () => {
    stopCamera();
});

function stopCamera() {
    // Stop detection
    if (detectionInterval) {
        clearInterval(detectionInterval);
        detectionInterval = null;
    }

    if (cameraStream) {
        cameraStream.getTracks().forEach(track => track.stop());
        cameraStream = null;
    }

    const videoElement = document.getElementById('cameraVideo');
    videoElement.srcObject = null;

    // Clear canvas
    const canvas = document.getElementById('cameraCanvas');
    const ctx = canvas.getContext('2d');
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    // Reset focus tracking
    cameraFocusHistory = [];
    cameraTotalFocusedTime = 0;
    cameraTotalDistractedTime = 0;
    cameraDistractedEvents = 0;
    cameraCurrentlyDistracted = false;
    cameraStartTime = null;

    // Update UI
    document.getElementById('cameraStartBtn').disabled = false;
    document.getElementById('cameraStopBtn').disabled = true;
    document.getElementById('cameraRecordBtn').disabled = true;
    document.getElementById('cameraStatus').textContent = 'Ch∆∞a b·∫≠t';
    document.getElementById('cameraStatus').style.color = '#888';
    document.getElementById('cameraFaceCount').textContent = '0';
    document.getElementById('cameraEmotion').textContent = '-';
    document.getElementById('cameraFocusScore').textContent = '-';
    document.getElementById('cameraFocusedTime').textContent = '0s';
    document.getElementById('cameraDistractedTime').textContent = '0s';
    document.getElementById('cameraDistractedCount').textContent = '0';

    showStatus('cameraStatusMsg', '‚èπ Camera ƒë√£ t·∫Øt', 'info');
}

// Start Recording
document.getElementById('cameraRecordBtn').addEventListener('click', () => {
    startCameraRecording();
});

// Stop Recording
document.getElementById('cameraStopRecordBtn').addEventListener('click', () => {
    stopCameraRecording();
});

function startCameraRecording() {
    if (!cameraStream) return;

    cameraRecordedChunks = [];

    // Create MediaRecorder
    try {
        cameraMediaRecorder = new MediaRecorder(cameraStream, {
            mimeType: 'video/webm;codecs=vp9'
        });
    } catch (e) {
        // Fallback to default codec
        cameraMediaRecorder = new MediaRecorder(cameraStream);
    }

    cameraMediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
            cameraRecordedChunks.push(event.data);
        }
    };

    cameraMediaRecorder.onstop = () => {
        // Create blob and upload
        const blob = new Blob(cameraRecordedChunks, { type: 'video/webm' });
        uploadCameraRecording(blob);
    };

    cameraMediaRecorder.start();
    cameraRecordingStartTime = Date.now();

    // Update UI
    document.getElementById('cameraRecordBtn').style.display = 'none';
    document.getElementById('cameraStopRecordBtn').style.display = 'inline-block';
    document.getElementById('cameraRecordingIndicator').style.display = 'block';
    document.getElementById('cameraTimer').style.display = 'block';

    // Start timer
    cameraTimerInterval = setInterval(updateCameraTimer, 1000);

    showStatus('cameraStatusMsg', '‚è∫ ƒêang ghi h√¨nh...', 'info');
}

function stopCameraRecording() {
    if (cameraMediaRecorder && cameraMediaRecorder.state !== 'inactive') {
        cameraMediaRecorder.stop();
    }

    // Stop timer
    if (cameraTimerInterval) {
        clearInterval(cameraTimerInterval);
        cameraTimerInterval = null;
    }

    // Update UI
    document.getElementById('cameraRecordBtn').style.display = 'inline-block';
    document.getElementById('cameraStopRecordBtn').style.display = 'none';
    document.getElementById('cameraRecordingIndicator').style.display = 'none';

    showStatus('cameraStatusMsg', '‚èπ ƒê√£ d·ª´ng ghi. ƒêang upload v√† ph√¢n t√≠ch...', 'info');
}

function updateCameraTimer() {
    if (!cameraRecordingStartTime) return;

    const elapsed = Math.floor((Date.now() - cameraRecordingStartTime) / 1000);
    const minutes = Math.floor(elapsed / 60);
    const seconds = elapsed % 60;

    document.getElementById('cameraTimerValue').textContent =
        `${String(minutes).padStart(2, '0')}:${String(seconds).padStart(2, '0')}`;
}

async function uploadCameraRecording(blob) {
    try {
        showStatus('cameraStatusMsg', 'üì§ ƒêang upload v√† ph√¢n t√≠ch video...', 'info');

        const formData = new FormData();
        formData.append('file', blob, 'camera-recording.webm');

        const response = await fetch(`${API_URL}/api/analyze-sync`, {
            method: 'POST',
            body: formData
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}`);
        }

        const result = await response.json();

        showStatus('cameraStatusMsg', '‚úÖ Ph√¢n t√≠ch ho√†n t·∫•t!', 'success');
        displayCameraResults(result);

    } catch (error) {
        console.error('Upload error:', error);
        showStatus('cameraStatusMsg', `‚ùå L·ªói khi upload: ${error.message}`, 'error');
    }
}

function displayCameraResults(result) {
    // Extract focus details
    const focusDetails = result.details && result.details.focus ? result.details.focus : {};

    // Show results in alert with focus details
    const message = `üìä K·∫øt Qu·∫£ Ph√¢n T√≠ch Camera:

ƒêi·ªÉm T·ªïng: ${result.scores.total.toFixed(1)}/10
Rating: ${result.rating}

Chi ti·∫øt:
- C·∫£m x√∫c: ${result.scores.emotion.toFixed(1)}/10
- T·∫≠p trung: ${result.scores.focus.toFixed(1)}/10
- R√µ r√†ng: ${result.scores.clarity.toFixed(1)}/10
- N·ªôi dung: ${result.scores.content.toFixed(1)}/10

üìä Chi ti·∫øt t·∫≠p trung:
- Th·ªùi gian t·∫≠p trung: ${focusDetails.focused_time || 0}s (${focusDetails.focused_rate || 0}%)
- Th·ªùi gian m·∫•t t·∫≠p trung: ${focusDetails.distracted_time || 0}s (${focusDetails.distracted_rate || 0}%)
- S·ªë l·∫ßn m·∫•t t·∫≠p trung: ${focusDetails.distracted_count || 0} l·∫ßn
- ƒêi·ªÉm trung b√¨nh: ${focusDetails.average_attention || 0}/10`;

    alert(message);
}

// Real-time face detection with face-api.js
async function startRealTimeFaceDetection() {
    const video = document.getElementById('cameraVideo');
    const canvas = document.getElementById('cameraCanvas');

    // Set canvas size to match video
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;

    const displaySize = { width: video.videoWidth, height: video.videoHeight };
    faceapi.matchDimensions(canvas, displaySize);

    // Start tracking time
    cameraStartTime = Date.now();

    // Detection loop
    detectionInterval = setInterval(async () => {
        if (!cameraStream) {
            clearInterval(detectionInterval);
            return;
        }

        try {
            // Detect faces with expressions
            const detections = await faceapi
                .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                .withFaceExpressions();

            // Calculate focus score (0-10)
            let focusScore = 0;

            if (detections && detections.length > 0) {
                // Clear canvas
                const ctx = canvas.getContext('2d');
                ctx.clearRect(0, 0, canvas.width, canvas.height);

                // Resize detections to match display
                const resizedDetections = faceapi.resizeResults(detections, displaySize);

                // Draw detections
                resizedDetections.forEach(detection => {
                    const box = detection.detection.box;

                    // Draw bounding box
                    ctx.strokeStyle = '#43e97b';
                    ctx.lineWidth = 3;
                    ctx.strokeRect(box.x, box.y, box.width, box.height);

                    // Get dominant emotion
                    const expressions = detection.expressions;
                    const dominantEmotion = Object.keys(expressions).reduce((a, b) =>
                        expressions[a] > expressions[b] ? a : b
                    );
                    const confidence = (expressions[dominantEmotion] * 100).toFixed(0);

                    // Draw emotion label
                    const emotionEmoji = getEmotionEmojiFromName(dominantEmotion);
                    const label = `${emotionEmoji} ${dominantEmotion} (${confidence}%)`;

                    ctx.fillStyle = '#43e97b';
                    ctx.font = 'bold 20px Arial';
                    ctx.fillText(label, box.x, box.y - 10);
                });

                // Update stats
                document.getElementById('cameraFaceCount').textContent = detections.length;

                // Show dominant emotion of first face
                const firstFace = detections[0];
                const expressions = firstFace.expressions;
                const dominantEmotion = Object.keys(expressions).reduce((a, b) =>
                    expressions[a] > expressions[b] ? a : b
                );
                const emotionEmoji = getEmotionEmojiFromName(dominantEmotion);

                document.getElementById('cameraEmotion').textContent = emotionEmoji;
                document.getElementById('cameraEmotion').style.fontSize = '2em';

                // Calculate focus score based on face position and size
                const box = firstFace.detection.box;
                const centerX = box.x + box.width / 2;
                const centerY = box.y + box.height / 2;
                const videoCenterX = displaySize.width / 2;
                const videoCenterY = displaySize.height / 2;

                // Calculate deviation from center (0-1)
                const deviationX = Math.abs(centerX - videoCenterX) / (displaySize.width / 2);
                const deviationY = Math.abs(centerY - videoCenterY) / (displaySize.height / 2);
                const maxDeviation = Math.max(deviationX, deviationY);

                // Calculate face size ratio (ideal: 0.3-0.5 of frame)
                const faceArea = box.width * box.height;
                const frameArea = displaySize.width * displaySize.height;
                const sizeRatio = faceArea / frameArea;

                // Focus score components (0-10 scale)
                const positionScore = (1 - Math.min(maxDeviation, 1)) * 10; // 10 if centered
                const sizeScore = sizeRatio > 0.1 && sizeRatio < 0.6 ? 10 : 5; // 10 if good size

                // Combined focus score
                focusScore = (positionScore * 0.7 + sizeScore * 0.3);

                // Track focused/distracted time
                if (focusScore >= 6.0) {
                    cameraTotalFocusedTime += 0.1; // 100ms interval
                    if (cameraCurrentlyDistracted) {
                        cameraCurrentlyDistracted = false;
                    }
                } else {
                    cameraTotalDistractedTime += 0.1;
                    if (!cameraCurrentlyDistracted) {
                        cameraCurrentlyDistracted = true;
                        cameraDistractedEvents++;
                    }
                }

            } else {
                // No face detected - count as distracted
                const ctx = canvas.getContext('2d');
                ctx.clearRect(0, 0, canvas.width, canvas.height);

                document.getElementById('cameraFaceCount').textContent = '0';
                document.getElementById('cameraEmotion').textContent = '-';
                document.getElementById('cameraEmotion').style.fontSize = '1.5em';

                focusScore = 0;
                cameraTotalDistractedTime += 0.1;
                if (!cameraCurrentlyDistracted) {
                    cameraCurrentlyDistracted = true;
                    cameraDistractedEvents++;
                }
            }

            // Update focus UI
            cameraFocusHistory.push(focusScore);
            if (cameraFocusHistory.length > 30) {
                cameraFocusHistory.shift(); // Keep last 30 samples (3 seconds)
            }

            const avgFocusScore = cameraFocusHistory.reduce((a, b) => a + b, 0) / cameraFocusHistory.length;
            document.getElementById('cameraFocusScore').textContent = avgFocusScore.toFixed(1);
            document.getElementById('cameraFocusedTime').textContent = `${cameraTotalFocusedTime.toFixed(1)}s`;
            document.getElementById('cameraDistractedTime').textContent = `${cameraTotalDistractedTime.toFixed(1)}s`;
            document.getElementById('cameraDistractedCount').textContent = cameraDistractedEvents;

            // Color code focus score
            const focusScoreEl = document.getElementById('cameraFocusScore');
            if (avgFocusScore >= 7.5) {
                focusScoreEl.style.color = '#43e97b'; // Green - focused
            } else if (avgFocusScore >= 6.0) {
                focusScoreEl.style.color = '#fee140'; // Yellow - slightly distracted
            } else {
                focusScoreEl.style.color = '#f5576c'; // Red - distracted
            }

        } catch (error) {
            console.error('Detection error:', error);
        }

    }, 100); // Detect every 100ms (10 FPS)
}

function getEmotionEmojiFromName(emotion) {
    const emojiMap = {
        'happy': 'üòä',
        'sad': 'üò¢',
        'angry': 'üò†',
        'fearful': 'üò®',
        'disgusted': 'ü§¢',
        'surprised': 'üò≤',
        'neutral': 'üòê'
    };
    return emojiMap[emotion] || 'üòê';
}

// Cleanup camera on page unload
window.addEventListener('beforeunload', () => {
    if (cameraStream) {
        cameraStream.getTracks().forEach(track => track.stop());
    }
    if (detectionInterval) {
        clearInterval(detectionInterval);
    }
});

// ===== CHECK API ON LOAD =====
window.addEventListener('load', async () => {
    try {
        const response = await fetch(`${API_URL}/health`);
        if (!response.ok) {
            throw new Error('API not responding');
        }
        console.log('‚úÖ API is running');
    } catch (error) {
        console.error('‚ùå API not available:', error);
        alert('‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi API.\n\nVui l√≤ng ch·∫°y: python api/main.py');
    }
});


// ===== TAB 5: DUAL PERSON COMPARISON =====
let dualPerson1Stream = null;
let dualPerson2Stream = null;
let dualComparisonInterval = null;

document.getElementById('dualStartBtn').addEventListener('click', async () => {
    try {
        showStatus('dualStatus', '‚è≥ ƒêang kh·ªüi ƒë·ªông camera v√† screen capture...', 'info');

        // Start camera for person 1
        dualPerson1Stream = await navigator.mediaDevices.getUserMedia({
            video: { width: { ideal: 1280 }, height: { ideal: 720 }, facingMode: 'user' },
            audio: false
        });

        const video1 = document.getElementById('dualPerson1Video');
        video1.srcObject = dualPerson1Stream;

        // Start screen capture for person 2
        dualPerson2Stream = await navigator.mediaDevices.getDisplayMedia({
            video: { width: { ideal: 1280 }, height: { ideal: 720 } },
            audio: false
        });

        const canvas2 = document.getElementById('dualPerson2Canvas');
        const video2 = document.createElement('video');
        video2.srcObject = dualPerson2Stream;
        video2.play();

        // Draw screen capture to canvas
        const ctx2 = canvas2.getContext('2d');
        const drawScreen = () => {
            if (dualPerson2Stream && dualPerson2Stream.active) {
                canvas2.width = video2.videoWidth;
                canvas2.height = video2.videoHeight;
                ctx2.drawImage(video2, 0, 0, canvas2.width, canvas2.height);
                requestAnimationFrame(drawScreen);
            }
        };
        video2.onloadedmetadata = () => {
            drawScreen();
        };

        // Update UI
        document.getElementById('dualStartBtn').disabled = true;
        document.getElementById('dualStopBtn').disabled = false;
        document.getElementById('dualExportBtn').disabled = false;

        showStatus('dualStatus', '‚úÖ ƒêang so s√°nh 2 ng∆∞·ªùi...', 'success');

        // Start comparison updates
        dualComparisonInterval = setInterval(updateDualComparison, 2000);

    } catch (error) {
        console.error('Dual person error:', error);
        showStatus('dualStatus', `‚ùå L·ªói: ${error.message}`, 'error');
    }
});

document.getElementById('dualStopBtn').addEventListener('click', () => {
    stopDualPerson();
});

document.getElementById('dualExportBtn').addEventListener('click', () => {
    alert('üìä Ch·ª©c nƒÉng xu·∫•t b√°o c√°o ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn!');
});

function stopDualPerson() {
    if (dualPerson1Stream) {
        dualPerson1Stream.getTracks().forEach(track => track.stop());
        dualPerson1Stream = null;
    }

    if (dualPerson2Stream) {
        dualPerson2Stream.getTracks().forEach(track => track.stop());
        dualPerson2Stream = null;
    }

    if (dualComparisonInterval) {
        clearInterval(dualComparisonInterval);
        dualComparisonInterval = null;
    }

    document.getElementById('dualStartBtn').disabled = false;
    document.getElementById('dualStopBtn').disabled = true;
    document.getElementById('dualExportBtn').disabled = true;

    showStatus('dualStatus', '‚èπ ƒê√£ d·ª´ng so s√°nh', 'info');
}

function updateDualComparison() {
    // Simulated comparison (in real app, would use face-api.js)
    const emotions = ['happy', 'neutral', 'sad', 'surprised'];
    const person1Emotion = emotions[Math.floor(Math.random() * emotions.length)];
    const person2Emotion = emotions[Math.floor(Math.random() * emotions.length)];

    document.getElementById('dualPerson1Emotion').textContent = person1Emotion;
    document.getElementById('dualPerson2Emotion').textContent = person2Emotion;

    const comparisonText = `
        Ng∆∞·ªùi 1: ${person1Emotion} | Ng∆∞·ªùi 2: ${person2Emotion}
        ${person1Emotion === person2Emotion ? '‚úÖ C·∫£m x√∫c gi·ªëng nhau' : '‚ö†Ô∏è C·∫£m x√∫c kh√°c nhau'}
    `;

    document.getElementById('dualComparisonResults').textContent = comparisonText;
}

// ===== TAB 7: VIDEO CALL =====
// Video call functionality is in separate videocall.html/videocall.js
// This tab just provides a link to open the video call interface

// ===== AUDIO RECORDING =====
let audioRecorder = null;
let audioRecordedChunks = [];
let audioRecordingStartTime = null;
let audioTimerInterval = null;

document.getElementById('audioStartRecordBtn').addEventListener('click', async () => {
    try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        audioRecorder = new MediaRecorder(stream);
        audioRecordedChunks = [];

        audioRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
                audioRecordedChunks.push(event.data);
            }
        };

        audioRecorder.onstop = () => {
            const blob = new Blob(audioRecordedChunks, { type: 'audio/webm' });
            const url = URL.createObjectURL(blob);

            const audioPlayback = document.getElementById('audioPlayback');
            audioPlayback.src = url;
            audioPlayback.style.display = 'block';

            document.getElementById('audioPlayBtn').disabled = false;
        };

        audioRecorder.start();
        audioRecordingStartTime = Date.now();

        document.getElementById('audioStartRecordBtn').disabled = true;
        document.getElementById('audioStopRecordBtn').disabled = false;
        document.getElementById('audioRecordTimer').style.display = 'block';

        audioTimerInterval = setInterval(updateAudioTimer, 1000);

        showStatus('audioStatus', '‚è∫ ƒêang thu √¢m...', 'info');

    } catch (error) {
        console.error('Audio recording error:', error);
        showStatus('audioStatus', `‚ùå L·ªói: ${error.message}`, 'error');
    }
});

document.getElementById('audioStopRecordBtn').addEventListener('click', () => {
    if (audioRecorder && audioRecorder.state !== 'inactive') {
        audioRecorder.stop();
        audioRecorder.stream.getTracks().forEach(track => track.stop());
    }

    if (audioTimerInterval) {
        clearInterval(audioTimerInterval);
        audioTimerInterval = null;
    }

    document.getElementById('audioStartRecordBtn').disabled = false;
    document.getElementById('audioStopRecordBtn').disabled = true;
    document.getElementById('audioRecordTimer').style.display = 'none';

    showStatus('audioStatus', '‚úÖ ƒê√£ d·ª´ng thu √¢m', 'success');
});

document.getElementById('audioPlayBtn').addEventListener('click', () => {
    const audioPlayback = document.getElementById('audioPlayback');
    audioPlayback.play();
});

function updateAudioTimer() {
    if (!audioRecordingStartTime) return;

    const elapsed = Math.floor((Date.now() - audioRecordingStartTime) / 1000);
    const minutes = Math.floor(elapsed / 60);
    const seconds = elapsed % 60;

    document.getElementById('audioRecordTimerValue').textContent =
        `${String(minutes).padStart(2, '0')}:${String(seconds).padStart(2, '0')}`;
}

// Cleanup on page unload
window.addEventListener('beforeunload', () => {
    stopDualPerson();

    if (audioRecorder && audioRecorder.state !== 'inactive') {
        audioRecorder.stop();
    }
});
