"""
Attention Detector Module

Ph√°t hi·ªán m·ª©c ƒë·ªô t·∫≠p trung theo h·ªá th·ªëng m·ªõi (THANG ƒêI·ªÇM 0-10):

C√îNG TH·ª®C CH√çNH:
FocusScore = (AvgScore + MinScore) / 2

Trong ƒë√≥:
- AvgScore: ƒêi·ªÉm trung b√¨nh (0-10)
- MinScore: ƒêi·ªÉm th·∫•p nh·∫•t (0-10) - ph√°t hi·ªán m·∫•t t·∫≠p trung nghi√™m tr·ªçng

ƒêI·ªÇM T·ª®C TH·ªúI (InstantScore):
InstantScore = FTS √ó 40% + EGA √ó 40% + MS √ó 20%

- FTS (Face Tracking Score): 40% - g√≥c quay ƒë·∫ßu (yaw, pitch, roll)
- EGA (Eye Gaze Alignment): 40% - tr·∫°ng th√°i m·∫Øt v√† h∆∞·ªõng nh√¨n
- MS (Movement Stability): 20% - ·ªïn ƒë·ªãnh chuy·ªÉn ƒë·ªông

THANG ƒêI·ªÇM 0-10:
- 8-10: T·∫≠p trung t·ªët (Focused)
- 6-8: H∆°i m·∫•t t·∫≠p trung (Slightly Distracted)
- 4-6: M·∫•t t·∫≠p trung (Distracted)
- 0-4: R·∫•t m·∫•t t·∫≠p trung (Very Distracted)

C√ÅC ƒêI·ªÄU CH·ªàNH:
1. ƒêi·ªÅu ch·ªânh tr·ªçng s·ªë FTS/EGA/MS (40-40-20)
2. Th√™m ƒëi·ªÉm trung v·ªã + ƒëi·ªÉm th·∫•p nh·∫•t ‚Üí h·∫°n ch·∫ø nhi·ªÖu
3. Ph√¢n lo·∫°i m·ª©c ƒë·ªô m·∫•t t·∫≠p trung (nh·∫π/v·ª´a/n·∫∑ng)
4. X·ª≠ l√Ω m·∫•t t·∫≠p trung k√©o d√†i (continuous lapse)
5. Tr·ªçng s·ªë camera/screen theo ng·ªØ c·∫£nh
6. T·ª± nh·∫≠n di·ªán ch·∫•t l∆∞·ª£ng camera ƒë·ªÉ gi·∫£m bias
7. Thang ƒëi·ªÉm 0-10 th·ªëng nh·∫•t v·ªõi h·ªá th·ªëng c·∫£m x√∫c
"""

import cv2
import numpy as np
from typing import Tuple, Optional, Dict
from collections import deque
import time


class AttentionLevel:
    """M·ª©c ƒë·ªô t·∫≠p trung (thang 0-10)."""
    FOCUSED = "focused"           # ƒêang t·∫≠p trung (8-10)
    SLIGHTLY_DISTRACTED = "slightly_distracted"  # H∆°i m·∫•t t·∫≠p trung (6-8)
    DISTRACTED = "distracted"     # M·∫•t t·∫≠p trung (4-6)
    VERY_DISTRACTED = "very_distracted"  # R·∫•t m·∫•t t·∫≠p trung (0-4)


class AttentionDetector:
    """
    Ph√°t hi·ªán m·ª©c ƒë·ªô t·∫≠p trung t·ª´ khu√¥n m·∫∑t v√† m·∫Øt.
    
    ƒê√°nh gi√° d·ª±a tr√™n:
    - Eye Aspect Ratio (EAR) - ph√°t hi·ªán m·∫Øt nh·∫Øm
    - Gaze direction - h∆∞·ªõng nh√¨n
    - Head pose - h∆∞·ªõng ƒë·∫ßu
    - Blink frequency - t·∫ßn su·∫•t ch·ªõp m·∫Øt
    """
    
    def __init__(
        self,
        ear_threshold: float = 0.15,
        gaze_threshold: float = 0.45,
        pose_threshold: float = 25.0,
        history_size: int = 30,
        alert_duration: float = 4.0
    ):
        """
        Initialize AttentionDetector.
        
        Args:
            ear_threshold: Ng∆∞·ª°ng EAR ƒë·ªÉ ph√°t hi·ªán m·∫Øt nh·∫Øm (0.15 - gi·∫£m ƒë·ªÉ √≠t nh·∫°y c·∫£m h∆°n)
            gaze_threshold: Ng∆∞·ª°ng gaze ƒë·ªÉ ph√°t hi·ªán nh√¨n ra ngo√†i (0.45 - tƒÉng m·∫°nh ƒë·ªÉ gi·∫£m false positive)
            pose_threshold: Ng∆∞·ª°ng g√≥c ƒë·∫ßu (ƒë·ªô) - 25¬∞ ƒë·ªÉ gi·∫£m nh·∫°y c·∫£m
            history_size: S·ªë frame l∆∞u l·ªãch s·ª≠
            alert_duration: Th·ªùi gian m·∫•t t·∫≠p trung tr∆∞·ªõc khi c·∫£nh b√°o (gi√¢y) - tƒÉng l√™n 4s
        """
        self.ear_threshold = ear_threshold
        self.gaze_threshold = gaze_threshold
        self.pose_threshold = pose_threshold
        self.history_size = history_size
        self.alert_duration = alert_duration
        
        # Dead zone - v√πng coi nh∆∞ ƒëang nh√¨n th·∫≥ng (gi·∫£m nhi·ªÖu) - TƒÇNG M·∫†NH
        self.pose_dead_zone = 15.0  # G√≥c < 15¬∞ = nh√¨n th·∫≥ng (tƒÉng t·ª´ 10¬∞)
        self.gaze_dead_zone = 0.25  # Gaze deviation < 0.25 = nh√¨n th·∫≥ng (tƒÉng t·ª´ 0.15)
        
        # History
        self.attention_history = deque(maxlen=history_size)
        self.ear_history = deque(maxlen=history_size)
        
        # Statistics
        self.total_frames = 0
        self.focused_frames = 0
        self.distracted_frames = 0
        self.eyes_closed_frames = 0
        self.looking_away_frames = 0
        
        # Alert tracking
        self.distraction_start_time = None
        self.last_alert_time = 0
        
        # Blink detection
        self.blink_counter = 0
        self.last_blink_time = 0
    
    def calculate_ear(self, eye_landmarks: np.ndarray) -> float:
        """
        T√≠nh Eye Aspect Ratio (EAR).
        
        EAR = (||p2-p6|| + ||p3-p5||) / (2 * ||p1-p4||)
        
        Args:
            eye_landmarks: 6 ƒëi·ªÉm landmarks c·ªßa m·∫Øt (ho·∫∑c 1 ƒëi·ªÉm cho MTCNN)
            
        Returns:
            EAR value (0-1, th·∫•p = m·∫Øt nh·∫Øm)
        """
        # For MTCNN (only 1 point per eye), we can't calculate EAR
        # Return a default value indicating eyes are open
        if len(eye_landmarks) < 6:
            return 0.3  # Default value (eyes open)
        
        # Vertical distances
        v1 = np.linalg.norm(eye_landmarks[1] - eye_landmarks[5])
        v2 = np.linalg.norm(eye_landmarks[2] - eye_landmarks[4])
        
        # Horizontal distance
        h = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])
        
        if h == 0:
            return 0.3
        
        ear = (v1 + v2) / (2.0 * h)
        return ear
    
    def extract_eye_landmarks(
        self,
        landmarks: np.ndarray,
        eye: str = "left"
    ) -> Optional[np.ndarray]:
        """
        Tr√≠ch xu·∫•t landmarks c·ªßa m·∫Øt.
        
        Args:
            landmarks: Full facial landmarks
            eye: "left" ho·∫∑c "right"
            
        Returns:
            6 ƒëi·ªÉm landmarks c·ªßa m·∫Øt ho·∫∑c None (ho·∫∑c 1 ƒëi·ªÉm cho MTCNN)
        """
        if len(landmarks) == 5:  # MTCNN format (5 points)
            # MTCNN: [left_eye, right_eye, nose, left_mouth, right_mouth]
            if eye == "left":
                return landmarks[0:1]  # Left eye point
            else:
                return landmarks[1:2]  # Right eye point
        
        elif len(landmarks) >= 468:  # MediaPipe format
            if eye == "left":
                # Left eye landmarks in MediaPipe
                indices = [33, 160, 158, 133, 153, 144]
            else:
                # Right eye landmarks
                indices = [362, 385, 387, 263, 373, 380]
            
            if len(landmarks) > max(indices):
                return landmarks[indices]
        
        elif len(landmarks) >= 68:  # dlib format
            if eye == "left":
                # Left eye: 36-41
                return landmarks[36:42]
            else:
                # Right eye: 42-47
                return landmarks[42:48]
        
        return None
    
    def detect_eyes_closed(self, landmarks: np.ndarray) -> Tuple[bool, float]:
        """
        Ph√°t hi·ªán m·∫Øt nh·∫Øm.
        
        Args:
            landmarks: Facial landmarks
            
        Returns:
            Tuple of (eyes_closed, average_ear)
        """
        # For MTCNN (5 points), we can't reliably detect eye closure
        # Return default values (eyes open)
        if len(landmarks) == 5:
            return False, 0.3
        
        # Extract eye landmarks
        left_eye = self.extract_eye_landmarks(landmarks, "left")
        right_eye = self.extract_eye_landmarks(landmarks, "right")
        
        if left_eye is None or right_eye is None:
            return False, 0.3
        
        # Calculate EAR for both eyes
        left_ear = self.calculate_ear(left_eye)
        right_ear = self.calculate_ear(right_eye)
        
        avg_ear = (left_ear + right_ear) / 2.0
        
        # Store in history
        self.ear_history.append(avg_ear)
        
        # Eyes closed if EAR below threshold
        eyes_closed = avg_ear < self.ear_threshold
        
        # Detect blink (quick close and open)
        if eyes_closed and len(self.ear_history) > 1:
            if self.ear_history[-2] >= self.ear_threshold:
                # Just closed
                current_time = time.time()
                if current_time - self.last_blink_time > 0.2:  # Min 0.2s between blinks
                    self.blink_counter += 1
                    self.last_blink_time = current_time
        
        return eyes_closed, avg_ear
    
    def estimate_gaze_direction(
        self,
        landmarks: np.ndarray,
        frame_shape: Tuple[int, int]
    ) -> Tuple[float, float]:
        """
        ∆Ø·ªõc t√≠nh h∆∞·ªõng nh√¨n (ƒë∆°n gi·∫£n h√≥a).
        
        Args:
            landmarks: Facial landmarks
            frame_shape: (height, width)
            
        Returns:
            Tuple of (horizontal_ratio, vertical_ratio)
            - 0.5, 0.5 = nh√¨n th·∫≥ng
            - < 0.5 = nh√¨n tr√°i/l√™n
            - > 0.5 = nh√¨n ph·∫£i/xu·ªëng
        """
        h, w = frame_shape
        
        # For MTCNN (5 points)
        if len(landmarks) == 5:
            # Use eye positions directly
            left_eye = landmarks[0]
            right_eye = landmarks[1]
            eye_center = (left_eye + right_eye) / 2.0
            
            # Normalize to frame
            h_ratio = eye_center[0] / w if w > 0 else 0.5
            v_ratio = eye_center[1] / h if h > 0 else 0.5
            
            return h_ratio, v_ratio
        
        # Get eye centers for other formats
        left_eye = self.extract_eye_landmarks(landmarks, "left")
        right_eye = self.extract_eye_landmarks(landmarks, "right")
        
        if left_eye is None or right_eye is None:
            return 0.5, 0.5
        
        # Calculate eye centers
        left_center = np.mean(left_eye, axis=0)
        right_center = np.mean(right_eye, axis=0)
        eye_center = (left_center + right_center) / 2.0
        
        # Normalize to frame
        h_ratio = eye_center[0] / w if w > 0 else 0.5
        v_ratio = eye_center[1] / h if h > 0 else 0.5
        
        return h_ratio, v_ratio
    
    def calculate_head_pose(self, landmarks: np.ndarray) -> Tuple[float, float, float]:
        """
        T√≠nh g√≥c ƒë·∫ßu (yaw, pitch, roll).
        
        Args:
            landmarks: Facial landmarks
            
        Returns:
            Tuple of (yaw, pitch, roll) in degrees
        """
        # MTCNN format (5 points)
        if len(landmarks) == 5:
            # MTCNN: [left_eye, right_eye, nose, left_mouth, right_mouth]
            left_eye = landmarks[0]
            right_eye = landmarks[1]
            nose_tip = landmarks[2]
            left_mouth = landmarks[3]
            right_mouth = landmarks[4]
        
        # MediaPipe format
        elif len(landmarks) >= 468:
            nose_tip = landmarks[1]
            left_eye = landmarks[33]
            right_eye = landmarks[263]
            left_mouth = landmarks[61]
            right_mouth = landmarks[291]
        
        # dlib format
        elif len(landmarks) >= 68:
            nose_tip = landmarks[30]
            left_eye = landmarks[36]
            right_eye = landmarks[45]
            left_mouth = landmarks[48]
            right_mouth = landmarks[54]
        
        else:
            return 0.0, 0.0, 0.0
        
        # Calculate yaw (left-right rotation)
        eye_center = (left_eye + right_eye) / 2
        nose_to_left = np.linalg.norm(nose_tip - left_eye)
        nose_to_right = np.linalg.norm(nose_tip - right_eye)
        face_width = np.linalg.norm(right_eye - left_eye)
        
        if face_width > 0:
            # ƒê·∫¢O D·∫§U: nose g·∫ßn left = quay tr√°i (√¢m), nose g·∫ßn right = quay ph·∫£i (d∆∞∆°ng)
            yaw_ratio = (nose_to_left - nose_to_right) / face_width
            yaw = yaw_ratio * 90  # Scale to degrees
        else:
            yaw = 0.0
        
        # Calculate pitch (up-down rotation)
        mouth_center = (left_mouth + right_mouth) / 2
        eye_to_mouth = np.linalg.norm(mouth_center - eye_center)
        nose_to_eye = np.linalg.norm(nose_tip - eye_center)
        nose_to_mouth = np.linalg.norm(nose_tip - mouth_center)
        
        if eye_to_mouth > 0:
            # T√≠nh pitch d·ª±a tr√™n t·ª∑ l·ªá kho·∫£ng c√°ch
            # Nh√¨n l√™n: nose g·∫ßn eyes ‚Üí pitch √¢m
            # Nh√¨n xu·ªëng: nose g·∫ßn mouth ‚Üí pitch d∆∞∆°ng
            pitch_ratio = (nose_to_eye - nose_to_mouth) / eye_to_mouth
            pitch = pitch_ratio * 45  # Scale to degrees
        else:
            pitch = 0.0
        
        # Calculate roll (tilt)
        d_eye = right_eye - left_eye
        roll = np.degrees(np.arctan2(d_eye[1], d_eye[0]))
        
        return yaw, pitch, roll
    
    def calculate_attention_score(
        self,
        landmarks: np.ndarray,
        frame_shape: Tuple[int, int],
        face_detected: bool = True
    ) -> Tuple[float, Dict]:
        """
        T√≠nh ƒëi·ªÉm t·∫≠p trung (0-10) theo C√îNG TH·ª®C M·ªöI:
        
        FocusScore = (
            FacePresenceRatio * 0.40 +
            GazeFocus        * 0.30 +
            HeadFocus        * 0.20 +
            DriftScore       * 0.10
        ) * 10
        
        4 Th√†nh ph·∫ßn:
        1. Face Presence (40%): T·ª∑ l·ªá c√≥ m·∫∑t trong khung h√¨nh
        2. Gaze Focus (30%): Nh√¨n th·∫≥ng vs nh√¨n sang tr√°i/ph·∫£i/xu·ªëng
        3. Head Focus (20%): ƒê·∫ßu th·∫≥ng vs quay ƒë·∫ßu/c√∫i ƒë·∫ßu
        4. Drift Score (10%): Ph·∫°t khi ng√≥ nghi√™ng b·∫•t th∆∞·ªùng
        
        Args:
            landmarks: Facial landmarks (c√≥ th·ªÉ None n·∫øu kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c)
            frame_shape: (height, width)
            face_detected: True n·∫øu ph√°t hi·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t, False n·∫øu kh√¥ng
            
        Returns:
            Tuple of (score_0_10, details_dict)
        """
        self.total_frames += 1
        
        details = {}
        
        # ===== 1. FACE PRESENCE RATIO (40%) =====
        if not face_detected or landmarks is None or len(landmarks) == 0:
            # KH√îNG c√≥ m·∫∑t
            details['face_detected'] = False
            details['reason'] = "Khong phat hien khuon mat"
            self.distracted_frames += 1
            
            # Track face loss for drift detection
            if not hasattr(self, '_last_face_present'):
                self._last_face_present = True
            if self._last_face_present:
                # Face just disappeared - count as drift event
                if not hasattr(self, '_drift_events'):
                    self._drift_events = 0
                self._drift_events += 1
            self._last_face_present = False
            
            face_presence_score = 0.0
            gaze_focus_score = 0.0
            head_focus_score = 0.0
        else:
            # C√ì m·∫∑t
            details['face_detected'] = True
            self.focused_frames += 1
            self._last_face_present = True
            
            face_presence_score = 1.0
        
            # ===== 2. GAZE FOCUS (30%) =====
            h_ratio, v_ratio = self.estimate_gaze_direction(landmarks, frame_shape)
            details['gaze_h'] = h_ratio
            details['gaze_v'] = v_ratio
            
            # T√≠nh ƒë·ªô l·ªách t·ª´ trung t√¢m (0.5, 0.5)
            h_deviation = abs(h_ratio - 0.5)
            v_deviation = abs(v_ratio - 0.5)
            
            # Ng∆∞·ª°ng: l·ªách < 0.15 = nh√¨n th·∫≥ng, > 0.35 = nh√¨n xa
            if h_deviation < 0.15 and v_deviation < 0.15:
                gaze_focus_score = 1.0  # Nh√¨n th·∫≥ng
                details['gaze_status'] = "center"
            elif h_deviation > 0.35 or v_deviation > 0.35:
                gaze_focus_score = 0.0  # Nh√¨n xa
                if h_deviation > v_deviation:
                    details['gaze_status'] = "left" if h_ratio < 0.5 else "right"
                else:
                    details['gaze_status'] = "up" if v_ratio < 0.5 else "down"
            else:
                # Trung gian: t√≠nh t·ª∑ l·ªá
                max_deviation = max(h_deviation, v_deviation)
                gaze_focus_score = 1.0 - ((max_deviation - 0.15) / (0.35 - 0.15))
                gaze_focus_score = max(0.0, min(1.0, gaze_focus_score))
                details['gaze_status'] = "slight_off"
            
            # ===== 3. HEAD FOCUS (20%) =====
            yaw, pitch, roll = self.calculate_head_pose(landmarks)
            details['yaw'] = yaw
            details['pitch'] = pitch
            details['roll'] = roll
            
            # Ng∆∞·ª°ng: < 15¬∞ = th·∫≥ng, > 25¬∞ = quay ƒë·∫ßu
            max_angle = max(abs(yaw), abs(pitch), abs(roll))
            
            if max_angle < 15.0:
                head_focus_score = 1.0  # ƒê·∫ßu th·∫≥ng
                details['head_status'] = "straight"
            elif max_angle > 25.0:
                head_focus_score = 0.0  # Quay ƒë·∫ßu nhi·ªÅu
                if abs(yaw) == max_angle:
                    details['head_status'] = "turned_left" if yaw < 0 else "turned_right"
                elif abs(pitch) == max_angle:
                    details['head_status'] = "looking_down" if pitch > 0 else "looking_up"
                else:
                    details['head_status'] = "tilted"
            else:
                # Trung gian
                head_focus_score = 1.0 - ((max_angle - 15.0) / (25.0 - 15.0))
                head_focus_score = max(0.0, min(1.0, head_focus_score))
                details['head_status'] = "slight_turn"
            
            # T√≠nh EAR (ƒë·ªÉ hi·ªÉn th·ªã)
            is_mtcnn = len(landmarks) == 5
            if not is_mtcnn:
                eyes_closed, ear = self.detect_eyes_closed(landmarks)
                details['eyes_closed'] = eyes_closed
                details['ear'] = ear
            else:
                details['eyes_closed'] = False
                details['ear'] = 0.3
        
        # ===== 4. DRIFT SCORE (10%) =====
        # T√≠nh s·ªë l·∫ßn drift trong 60 gi√¢y g·∫ßn nh·∫•t
        if not hasattr(self, '_drift_events'):
            self._drift_events = 0
        
        # Cho ph√©p t·ªëi ƒëa 3 l·∫ßn drift/ph√∫t
        max_allowed_drifts = 3
        drift_score = 1.0 - min(1.0, self._drift_events / max_allowed_drifts)
        details['drift_events'] = self._drift_events
        details['drift_score'] = drift_score
        
        # Reset drift counter m·ªói 60 gi√¢y
        if not hasattr(self, '_last_drift_reset'):
            self._last_drift_reset = time.time()
        if time.time() - self._last_drift_reset > 60.0:
            self._drift_events = 0
            self._last_drift_reset = time.time()
        
        # ===== T√çNH ƒêI·ªÇM INSTANT (0-1) =====
        instant_score_normalized = (
            face_presence_score * 0.40 +
            gaze_focus_score * 0.30 +
            head_focus_score * 0.20 +
            drift_score * 0.10
        )
        
        # Chuy·ªÉn sang thang 0-10
        instant_score = instant_score_normalized * 10.0
        
        details['instant_score'] = instant_score
        details['face_presence_score'] = face_presence_score
        details['gaze_focus_score'] = gaze_focus_score
        details['head_focus_score'] = head_focus_score
        
        # X√°c ƒë·ªãnh l√Ω do m·∫•t t·∫≠p trung (n·∫øu c√≥)
        if instant_score < 7.0:
            if face_presence_score < 0.5:
                details['reason'] = "Khong co mat trong khung hinh"
            elif gaze_focus_score < 0.5:
                details['reason'] = f"Nhin {details.get('gaze_status', 'away')}"
            elif head_focus_score < 0.5:
                details['reason'] = f"Dau {details.get('head_status', 'turned')}"
            else:
                details['reason'] = "Ngo nghieng qua nhieu"
        else:
            details['reason'] = "Dang tap trung"
        
        # Store instant score
        self.attention_history.append(instant_score)
        
        # ===== 5. T√≠nh FocusScore (thang 0-10) =====
        # L√†m m∆∞·ª£t b·∫±ng c√°ch l·∫•y trung b√¨nh 30 frame g·∫ßn nh·∫•t (~1 gi√¢y ·ªü 30fps)
        if len(self.attention_history) >= 10:
            recent_scores = list(self.attention_history)[-30:]
            focus_score = np.mean(recent_scores)
            
            # ƒê·∫øm s·ªë frame m·∫•t t·∫≠p trung (score < 6.0)
            distracted_count = sum(1 for s in recent_scores if s < 6.0)
            distracted_ratio = distracted_count / len(recent_scores)
            
            # ƒê·∫øm s·ªë frame li√™n t·ª•c m·∫•t t·∫≠p trung
            continuous_lapse = 0
            for s in reversed(recent_scores):
                if s < 6.0:
                    continuous_lapse += 1
                else:
                    break
            
            details['avg_score'] = focus_score
            details['median_score'] = np.median(recent_scores)
            details['distracted_ratio'] = distracted_ratio
            details['continuous_lapse'] = continuous_lapse
            
            # C·∫£nh b√°o n·∫øu m·∫•t t·∫≠p trung k√©o d√†i >= 15 frame (~0.5s)
            if continuous_lapse >= 15:
                details['continuous_lapse_warning'] = True
        else:
            # Ch∆∞a ƒë·ªß l·ªãch s·ª≠ - d√πng instant score
            focus_score = instant_score
            details['avg_score'] = focus_score
            details['median_score'] = focus_score
            details['distracted_ratio'] = 0 if instant_score >= 6.0 else 1.0
            details['continuous_lapse'] = 0 if instant_score >= 6.0 else 1
        
        # Clamp score (0-10)
        final_score = max(0, min(10, focus_score))
        details['score'] = final_score
        
        # Reason ƒë√£ ƒë∆∞·ª£c set ·ªü tr√™n (d·ª±a v√†o face_detected)
        # Kh√¥ng c·∫ßn ki·ªÉm tra th√™m
        
        return final_score, details
    
    def get_attention_level(self, score: float) -> str:
        """L·∫•y m·ª©c ƒë·ªô t·∫≠p trung t·ª´ ƒëi·ªÉm (thang 0-10) - C√¥ng th·ª©c 4 th√†nh ph·∫ßn."""
        if score >= 7.5:
            return AttentionLevel.FOCUSED  # T·∫≠p trung t·ªët
        elif score >= 6.0:
            return AttentionLevel.SLIGHTLY_DISTRACTED  # H∆°i m·∫•t t·∫≠p trung
        elif score >= 4.0:
            return AttentionLevel.DISTRACTED  # M·∫•t t·∫≠p trung
        else:
            return AttentionLevel.VERY_DISTRACTED  # R·∫•t m·∫•t t·∫≠p trung
    
    def should_alert(self, score: float) -> bool:
        """
        Ki·ªÉm tra c√≥ n√™n c·∫£nh b√°o kh√¥ng (thang 0-10).
        
        Logic M·ªöI v·ªõi c√¥ng th·ª©c 4 th√†nh ph·∫ßn:
        - N·∫øu t·∫≠p trung t·ªët (score >= 7.0): T·∫ÆT c·∫£nh b√°o
        - N·∫øu m·∫•t t·∫≠p trung (score < 5.0): B·∫Øt ƒë·∫ßu ƒë·∫øm th·ªùi gian
        - N·∫øu m·∫•t t·∫≠p trung >= 3 gi√¢y: C·∫£nh b√°o
        - Cooldown: 3 gi√¢y gi·ªØa c√°c c·∫£nh b√°o
        
        Returns:
            True n·∫øu c·∫ßn c·∫£nh b√°o
        """
        current_time = time.time()
        
        # If focused (score >= 7.0), reset ALL timers and STOP alert
        if score >= 7.0:
            if self.distraction_start_time is not None:
                self.distraction_start_time = None
                # QUAN TR·ªåNG: Reset last_alert_time ƒë·ªÉ t·∫Øt c·∫£nh b√°o
                if self.last_alert_time > 0:
                    print(f"‚úì ƒê√£ t·∫≠p trung l·∫°i! Score: {score:.1f}/10 - T·∫ÆT c·∫£nh b√°o")
                self.last_alert_time = 0
            return False
        
        # If distracted (score < 5.0), start/continue timer
        if score < 5.0:
            if self.distraction_start_time is None:
                self.distraction_start_time = current_time
                print(f"‚ö†Ô∏è B·∫Øt ƒë·∫ßu m·∫•t t·∫≠p trung... Score: {score:.1f}/10")
                return False  # Ch∆∞a ƒë·ªß th·ªùi gian ƒë·ªÉ c·∫£nh b√°o
            
            # Check if distracted for too long
            distraction_duration = current_time - self.distraction_start_time
            
            # N·∫øu ƒë√£ m·∫•t t·∫≠p trung >= 3 gi√¢y
            if distraction_duration >= 3.0:
                # Check cooldown ƒë·ªÉ tr√°nh spam
                time_since_last_alert = current_time - self.last_alert_time
                
                if time_since_last_alert > 3.0:  # Cooldown 3 gi√¢y
                    self.last_alert_time = current_time
                    print(f"üö® C·∫¢NH B√ÅO: M·∫•t t·∫≠p trung {distraction_duration:.1f}s! Score: {score:.1f}/10")
                    # KH√îNG reset distraction_start_time - ƒë·ªÉ ti·∫øp t·ª•c c·∫£nh b√°o
                    return True
        else:
            # Score between 5.0 and 7.0 (slightly distracted) - monitor but don't alert
            if self.distraction_start_time is not None:
                # Reset timer n·∫øu ƒë√£ c·∫£i thi·ªán
                print(f"‚ÑπÔ∏è H∆°i m·∫•t t·∫≠p trung nh∆∞ng ch∆∞a nghi√™m tr·ªçng. Score: {score:.1f}/10")
                self.distraction_start_time = None
        
        return False
    
    def get_average_attention(self) -> float:
        """L·∫•y ƒëi·ªÉm t·∫≠p trung trung b√¨nh (thang 0-10)."""
        if len(self.attention_history) == 0:
            return 10.0
        return np.mean(list(self.attention_history))
    
    def get_statistics(self) -> Dict:
        """L·∫•y th·ªëng k√™."""
        if self.total_frames == 0:
            return {
                'total_frames': 0,
                'focused_rate': 0.0,
                'distracted_rate': 0.0,
                'eyes_closed_rate': 0.0,
                'looking_away_rate': 0.0,
                'average_attention': 100.0,
                'blink_count': 0
            }
        
        return {
            'total_frames': self.total_frames,
            'focused_frames': self.focused_frames,
            'distracted_frames': self.distracted_frames,
            'focused_rate': self.focused_frames / self.total_frames,
            'distracted_rate': self.distracted_frames / self.total_frames,
            'eyes_closed_rate': self.eyes_closed_frames / self.total_frames,
            'looking_away_rate': self.looking_away_frames / self.total_frames,
            'average_attention': self.get_average_attention(),
            'blink_count': self.blink_counter
        }
    
    def reset(self):
        """Reset statistics."""
        self.attention_history.clear()
        self.ear_history.clear()
        self.total_frames = 0
        self.focused_frames = 0
        self.distracted_frames = 0
        self.eyes_closed_frames = 0
        self.looking_away_frames = 0
        self.blink_counter = 0
        self.distraction_start_time = None
        self.last_alert_time = 0
